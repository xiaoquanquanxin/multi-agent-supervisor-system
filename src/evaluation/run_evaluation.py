from langsmith import Client
from dotenv import load_dotenv
import os
import asyncio
import pandas as pd
from tabulate import tabulate
import json
from datetime import datetime

from ..main import create_workflow
from .evaluators import (
    evaluate_task_completion, 
    check_node_execution,
    check_image_generation_node
)
from .create_dataset import create_evaluation_dataset

async def run_evaluations():
    # Initialize environment and check API key
    load_dotenv()
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ° OPENAI_API_KEY")
        return
        
    print("\nğŸš€ å¼€å§‹è¯„ä¼°æµç¨‹")
    print("==============================")
    
    # Step 1: Dataset Creation/Retrieval
    print("\n1ï¸âƒ£ è®¾ç½®æµ‹è¯•æ•°æ®é›†...")
    dataset = create_evaluation_dataset()
    client = Client()
    print("âœ“ æ•°æ®é›†å·²å‡†å¤‡å¥½ï¼Œæµ‹è¯•ç”¨ä¾‹ï¼šç”Ÿæˆå¸¦æ–‡æœ¬å åŠ çš„å›¾åƒ")
    
    # Step 2: Workflow Setup
    print("\n2ï¸âƒ£ åˆå§‹åŒ–å·¥ä½œæµ...")
    workflow = create_workflow()
    print("âœ“ å¤šæ™ºèƒ½ä½“å·¥ä½œæµå·²åˆå§‹åŒ–")
    
    # Step 3: Input Preparation
    print("\n3ï¸âƒ£ å‡†å¤‡è¾“å…¥å¤„ç†å™¨...")
    def process_request(inputs: dict) -> dict:
        return {
            "messages": [
                {"role": "user", "content": inputs["request"]}
            ],
            "next_agent": None,
            "current_task": None,
            "image_url": None,
            "processed_image_url": None
        }
    print("âœ“ è¾“å…¥å¤„ç†å™¨å·²å‡†å¤‡")
    
    # Step 4: Evaluation Setup
    print("\n4ï¸âƒ£ è®¾ç½®è¯„ä¼°...")
    target = process_request | workflow
    print("âœ“ è¯„ä¼°ç›®æ ‡å·²é…ç½®")
    
    # Step 5: Run Evaluation
    print("\n5ï¸âƒ£ è¿è¡Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°")
    print("=====================================")
    print("è¯„ä¼°ä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼š")
    print("1. ä»»åŠ¡å®Œæˆï¼šæ•´ä½“ç³»ç»Ÿæ€§èƒ½")
    print("2. èŠ‚ç‚¹æ‰§è¡Œï¼šæ™ºèƒ½ä½“äº¤äº’æ¨¡å¼")
    print("3. å•ä¸ªèŠ‚ç‚¹ï¼šç‰¹å®šæ™ºèƒ½ä½“æ€§èƒ½")
    
    experiment_results = await client.aevaluate(
        target,
        data=dataset.name,
        evaluators=[
            evaluate_task_completion,
            check_node_execution,
            check_image_generation_node
        ],
        experiment_prefix="image_processing_eval",
        num_repetitions=1,
        max_concurrency=1
    )
    print("âœ“ è¯„ä¼°å®Œæˆ")
    
    # Step 6: Process Results
    print("\n6ï¸âƒ£ å¤„ç†ç»“æœ...")
    results_df = experiment_results.to_pandas()
    
    results_dict = {
        "Test Request": {
            "input": results_df['inputs.request'].iloc[0],
            "expected_sequence": results_df['reference.expected_sequence'].iloc[0]
        },
        "Execution Results": {
            "agent_messages": results_df['outputs.messages'].iloc[0],
            "final_state": {
                "next_agent": results_df['outputs.next_agent'].iloc[0],
                "current_task": results_df['outputs.current_task'].iloc[0],
                "image_url": results_df.get('outputs.image_url', ['N/A']).iloc[0],
                "processed_image_url": results_df.get('outputs.processed_image_url', ['N/A']).iloc[0]
            }
        },
        "Evaluation": {
            "task_completion": {
                "score": float(results_df['feedback.evaluate_task_completion'].iloc[0]),
                "reasoning": str(results_df['feedback.evaluate_task_completion'].iloc[0])
            },
            "node_execution": {
                "score": float(results_df['feedback.check_node_execution'].iloc[0]),
                "reasoning": str(results_df['feedback.check_node_execution'].iloc[0])
            },
            "image_generation": {
                "score": float(results_df['feedback.check_image_generation_node'].iloc[0]),
                "reasoning": str(results_df['feedback.check_image_generation_node'].iloc[0])
            },
            "execution_time_seconds": float(results_df['execution_time'].iloc[0])
        }
    }
    
    # Step 7: Display Results
    print("\n7ï¸âƒ£ æŒ‰æ ‡å‡†åˆ†ç±»çš„è¯„ä¼°ç»“æœ")
    print("==============================")
    
    print("\n1ï¸âƒ£ ä»»åŠ¡å®Œæˆè¯„ä¼°ï¼š")
    print("   æ•´ä½“ç³»ç»Ÿæ€§èƒ½åˆ†æ•°")
    print(f"åˆ†æ•°ï¼š {results_dict['Evaluation']['task_completion']['score']}")
    print("åˆ†æï¼š")
    print(results_dict['Evaluation']['task_completion']['reasoning'])
    
    print("\n2ï¸âƒ£ èŠ‚ç‚¹æ‰§è¡Œåˆ†æï¼š")
    print("   æ™ºèƒ½ä½“äº¤äº’æ¨¡å¼åˆ†æ•°")
    print(f"åˆ†æ•°ï¼š {results_dict['Evaluation']['node_execution']['score']}")
    print("åˆ†æï¼š")
    print(results_dict['Evaluation']['node_execution']['reasoning'])
    
    print("\n3ï¸âƒ£ å›¾åƒç”ŸæˆèŠ‚ç‚¹æ£€æŸ¥ï¼š")
    print("   å•ä¸ªèŠ‚ç‚¹æ€§èƒ½åˆ†æ•°")
    print(f"åˆ†æ•°ï¼š {results_dict['Evaluation']['image_generation']['score']}")
    print("åˆ†æï¼š")
    print(results_dict['Evaluation']['image_generation']['reasoning'])
    
    # Step 8: Summary
    print("\n8ï¸âƒ£ å¿«é€Ÿæ¦‚è¿°")
    print("===============")
    print(f"â€¢ è¯·æ±‚ï¼š {results_dict['Test Request']['input']}")
    print(f"â€¢ ä»»åŠ¡å®Œæˆåˆ†æ•°ï¼š {results_dict['Evaluation']['task_completion']['score']}")
    print(f"â€¢ èŠ‚ç‚¹æ‰§è¡Œåˆ†æ•°ï¼š {results_dict['Evaluation']['node_execution']['score']}")
    print(f"â€¢ å›¾åƒç”Ÿæˆåˆ†æ•°ï¼š {results_dict['Evaluation']['image_generation']['score']}")
    print(f"â€¢ æ‰§è¡Œæ—¶é—´ï¼š {results_dict['Evaluation']['execution_time_seconds']:.2f} ç§’")
    
    return experiment_results

if __name__ == "__main__":
    asyncio.run(run_evaluations()) 